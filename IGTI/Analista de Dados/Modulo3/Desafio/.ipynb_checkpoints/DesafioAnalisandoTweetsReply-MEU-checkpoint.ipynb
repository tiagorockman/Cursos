{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pacotes\n",
    "import tweepy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from textblob import TextBlob as tb\n",
    "import time\n",
    "import re\n",
    "\n",
    "#pacotes para visualização \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#mapa\n",
    "from geopy.geocoders import Nominatim\n",
    "import folium\n",
    "from folium import plugins\n",
    "import pymongo\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#autenticacao tweet\n",
    "consumer_key = \"w14sE6WOPocTI0ZCmrXMoXLK2\"\n",
    "consumer_secret = \"IChDWYpSXRr5HFGxFKtoSUaEysiiY3LIIKVDrR4twEpk1QSZ9Y\"\n",
    "access_token = \"1574569164-EQGkgTBJhCIRwWPUWjCGZK7pkwV8P6ZKwDz1fSA\"\n",
    "access_token_secret = \"nrppar8dsdmQT1fFcGvYgbeBHu9JvcjIZzjPEpYOuzky8\"\n",
    "\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True, retry_count=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#definir listas de armazenamento\n",
    "#tweets = []\n",
    "info = []\n",
    "keyword = ('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SE já possui infor na base Mongo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define keyword usada na pesquisa para a base\n",
    "keyword = ('home office OR trabalho remoto OR remote work OR trabalho em casa OR remotework OR trabalhoremoto OR remote job')\n",
    "\n",
    "#conexao ao banco de dados\n",
    "conn = pymongo.MongoClient('localhost', 27017)\n",
    "\n",
    "#seleciona o banco de dados\n",
    "db = conn.igti\n",
    "#seleciona \n",
    "collection = db.desafio_twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#seleciona itens na base e armazena no array de tweets\n",
    "collectionGet = collection.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in collectionGet:\n",
    "    tweets = doc\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in collectionGet:\n",
    "    tweets.append(i)\n",
    "\n",
    "print(\"Total coletado: %s.\" % (len(tweets)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " from bson.json_util import dumps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweetsOrig = tweets\n",
    "tweets = dumps(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(tweetsOrig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(tweets)\n",
    "print(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets2 = json.load(tweetsOrig)\n",
    "type(tweets2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googletrans import Translator\n",
    "from unidecode import unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polaritiesPT = []\n",
    "polaritiesEN = []\n",
    "tweetsNew = []\n",
    "\n",
    "for tweet in tweets:\n",
    "    #Texto do Tweet\n",
    "    textReceived = tb(tweet.text)\n",
    "    if textReceived.detect_language() != 'en':\n",
    "           #print('** Tweet em português: '+textPT)\n",
    "        #Traduzindo para o Inglês\n",
    "        textEN = Translator().translate(textReceived)\n",
    "        tweet.text = textEN.text\n",
    "        #print('** Tweet traduzido:')\n",
    "        #print(textEN.text)\n",
    "        #Calculando a polaridade do texto traduzido\n",
    "        polarityPT = tb(textEN.text).sentiment.polarity #analisa a polaridade\n",
    "        polaritiesPT.append(polarityPT)\n",
    "    else:\n",
    "        polarityEN = tb(textReceived.text).sentiment.polarity\n",
    "        polaritiesEN.append(polarityEN)                \n",
    "     \n",
    "    if 'retweeted_status' in dir(tweet): #se for retweet pega o fullText\n",
    "        aux=tweet.retweeted_status.text            \n",
    "    else:\n",
    "        aux=tweet.text            \n",
    "        newtweet = aux.replace(\"\\n\", \" \")#onde for \\n substitui por espaco        \n",
    "        tweetsNew.append(newtweet) #adiciona ao vetor\n",
    "        info.append(tweet) #adiciona ao tweet\n",
    "    #print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"POLARIDADE PT: \" polaritiesPT)\n",
    "print(\"POLARIDADE EN: \" polaritiesEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#realiza loop nas informações\n",
    "for tweet in tweets:        \n",
    "        if 'retweeted_status' in dir(tweet): #se for retweet pega o fullText\n",
    "            aux=tweet.retweeted_status.full_text\n",
    "        else:\n",
    "            aux=tweet.full_text\n",
    "            \n",
    "        newtweet = aux.replace(\"\\n\", \" \")#onde for \\n substitui por espaco\n",
    "        \n",
    "        tweets.append(newtweet) #adiciona ao vetor\n",
    "        info.append(tweet) #adiciona ao tweet\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SE irá fazer a busca no Twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"tweets_Keyword_reply_Turma4.txt\", \"a\", -1, \"utf-8\")#abre o arquivo \n",
    "\n",
    "#busca por palavra chave\n",
    "for tweet in tweepy.Cursor(api.search,\n",
    "                            q=keyword, tweet_mode='extended',\n",
    "                           rpp=2500, result_type=\"popular\", lang='en', #serão solicitados apenas 200 tweet em ingles e popular\n",
    "                           include_entities=True).items(2500):\n",
    "                            #include_entities=True).items(1000);\n",
    "        \n",
    "        if 'retweeted_status' in dir(tweet): #se for retweet pega o fullText\n",
    "            aux=tweet.retweeted_status.full_text\n",
    "        else:\n",
    "            aux=tweet.full_text\n",
    "            \n",
    "        newtweet = aux.replace(\"\\n\", \" \")#onde for \\n substitui por espaco\n",
    "        \n",
    "        tweets.append(newtweet) #adiciona ao vetor\n",
    "        info.append(tweet) #adiciona ao tweet\n",
    "        \n",
    "        #file = open(\"tweets_Keyword_covid_10.txt\", \"a\", -1, \"utf-8\")\n",
    "        \n",
    "        file.write(newtweet+'\\n')\n",
    "\n",
    "file.close()#fecha\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#para verificar a quantidade de tweets coletados\n",
    "print(\"Total coletado: %s.\" % (len(info)))\n",
    "newtweet\n",
    "tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cria dataframe\n",
    "tweets_df = pd.DataFrame(tweets, columns=['Tweets']) \n",
    "\n",
    "tweets_df['len']  = np.array([len(tweet) for tweet in tweets])\n",
    "tweets_df['ID']   = np.array([tweet.id for tweet in info])\n",
    "tweets_df['USER']   = np.array([tweet.user.screen_name for tweet in info])\n",
    "tweets_df['userName'] = np.array([tweet.user.name for tweet in info])\n",
    "tweets_df['User Location']    = np.array([tweet.user.location for tweet in info])\n",
    "tweets_df['Language'] = np.array([tweet.user.lang for tweet in info])\n",
    "tweets_df['Date'] = np.array([tweet.created_at for tweet in info])\n",
    "tweets_df['Source'] = np.array([tweet.source for tweet in info])\n",
    "tweets_df['Likes']  = np.array([tweet.favorite_count for tweet in info])\n",
    "tweets_df['Retweets']    = np.array([tweet.retweet_count for tweet in info])\n",
    "tweets_df['Geo']    = np.array([tweet.geo for tweet in info])\n",
    "tweets_df['Coordinates']    = np.array([tweet.coordinates for tweet in info])         \n",
    "tweets_df['Place']    = np.array([tweet.place for tweet in info])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imprime as 3 primeiras linhas\n",
    "tweets_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#indica os Top tweets\n",
    "# Tweets com maior número de LIKES\n",
    "likes_max = np.max(tweets_df['Likes']) #Função max do numpy identifica o valor máximo\n",
    "\n",
    "likes = tweets_df[tweets_df.Likes == likes_max].index[0] #pega o primeiro tweet com valor máximo de curtidas\n",
    "\n",
    "print(\"O tweet com mais curtidas (likes) é: \\n{}\".format(tweets_df['Tweets'][likes]))\n",
    "print(\"Número de curtidas: {}\".format(likes_max))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.sum(tweets_df['Likes'] == likes_max)) #conta quantos tweets possuem o mesmo valor máximo de curtidas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retweet_max  = np.max(tweets_df['Retweets']) #retorna o valor máximo\n",
    "\n",
    "retweet  = tweets_df[tweets_df.Retweets == retweet_max].index[0] #pega o primeiro tweet com valor máximo de Retweets\n",
    "\n",
    "print(\"O tweet com mais retweet é: \\n{}\".format(tweets_df['Tweets'][retweet]))\n",
    "print(\"Número de retweets: {}\".format(retweet_max))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.sum(tweets_df['Retweets'] == retweet_max)) #conta quantos tweets possuem o mesmo valor máximo de Retweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ORIGEM DO TWEET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fonte / origem do tweet\n",
    "sources = [] #lista para armazenar a fontes\n",
    "for source in tweets_df['Source']:\n",
    "    if source not in sources:\n",
    "        sources.append(source) #inclui no vetor sources apenas se a fonte encontrada ainda não foi incluída\n",
    "\n",
    "percent = np.zeros(len(sources)) #Retorna um novo vetor, com o número de elementos do vetor sources, preenchido com zeros a new array filled with zeros, for\n",
    "\n",
    "for source in tweets_df['Source']:\n",
    "    for index in range(len(sources)):\n",
    "        if source == sources[index]:\n",
    "            percent[index] += 1\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sourceDF = pd.DataFrame({\n",
    " 'source':percent,\n",
    "}, index=sources)\n",
    "\n",
    "sourceDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sources_sorted = sourceDF.sort_values('source',ascending=True)\n",
    "ax = sources_sorted.source.plot(kind='barh',color='#A52A2A')\n",
    "ax.get_xaxis().set_major_formatter(plt.FuncFormatter(lambda x, loc: \"{:,}\".format(int(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sources_sorted = sourceDF.sort_values('source',ascending=False)\n",
    "ax = sources_sorted.source.plot(kind='barh',color='#B8860B')\n",
    "ax.get_xaxis().set_major_formatter(plt.FuncFormatter(lambda x, loc: \"{:,}\".format(int(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Variável que irá armazenar as polaridades\n",
    "analysis = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANALISE DE POLARIDADE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Variável que irá armazenar as polaridades\n",
    "analysis = None\n",
    "# Lista vazia para armazenar as polaridades\n",
    "polarities = []\n",
    "tweets_df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calcula a polaridade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tweet in tweets_df['Tweets']: #para cada tweet    \n",
    "    analysis = tb(tweet)   \n",
    "    \n",
    "    polarity = analysis.sentiment.polarity #analisa a polaridade\n",
    "\n",
    "    polarities.append(polarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Vetor de polaridade:',polarities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANALISE DE SENTIMENTO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Para a(s) palavra(s):\"%s\"' % keyword)\n",
    "print('A MÉDIA DE SENTIMENTO É: ' + str(np.mean(polarities)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive = 0\n",
    "negative = 0\n",
    "neutral = 0\n",
    "\n",
    "for polarity in polarities:\n",
    "    if polarity > 0:\n",
    "        positive = positive+1\n",
    "    elif polarity < 0:\n",
    "        negative = negative+1\n",
    "    else:\n",
    "        neutral = neutral+1  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Tweets Positivos: %s\" % positive)\n",
    "print(\"Tweets Negativos: %s\" % negative)\n",
    "print(\"Tweets Neutros: %s\" % neutral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calcula percentual\n",
    "pos_pct=positive*100/len(polarities)\n",
    "neg_pct=negative*100/len(polarities)\n",
    "neu_pct=neutral*100/len(polarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiments = ['Positivos', 'Negativos','Neutros']\n",
    "percents = [pos_pct, neg_pct, neu_pct]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pie_chart = pd.Series(percents, index=sentiments,name='Sentimentos')\n",
    "pie_chart.plot.pie(fontsize=12, autopct='%.2f', figsize=(5, 5),title=\"Análise de Sentimentos tweets\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ax1.pie(sizes, explode=explode, labels=labels, autopct='%1.1f%%',        shadow=True, startangle=90)\n",
    "explode = (0.1, 0, 0) #Separa o primeiro\n",
    "\n",
    "pie_chart = pd.Series(percents, index=sentiments,name='')\n",
    "pie_chart.plot.pie(fontsize=12, explode=explode, autopct='%.2f%%', shadow=True, figsize=(7, 7),title=\"Análise de Sentimentos tweets\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAPA DE CALOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geolocator = Nominatim(user_agent=\"TweeterSentiments\")\n",
    "\n",
    "latitude = []\n",
    "longitude = []\n",
    "\n",
    "for user_location in tweets_df['User Location']:\n",
    "    try:\n",
    "        location = geolocator.geocode(user_location)\n",
    "        latitude.append(location.latitude)\n",
    "        longitude.append(location.longitude)\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coordenadas = np.column_stack((latitude, longitude))\n",
    "\n",
    "mapa = folium.Map(zoom_start=3.)\n",
    "mapa.add_child(plugins.HeatMap(coordenadas))\n",
    "mapa.save('covid.html')\n",
    "mapa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NUVEM DE PALAVRAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ' '.join(tweets_df['Tweets'])\n",
    "\n",
    "words_clean = \" \".join([word for word in words.split()\n",
    "                            if 'https' not in word\n",
    "                                and not word.startswith('@')\n",
    "                                and word != 'RT'\n",
    "                            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "wc = WordCloud(min_font_size=10, \n",
    "               max_font_size=300, \n",
    "               background_color='white', \n",
    "               mode=\"RGB\",\n",
    "               width=2000, \n",
    "               height=1000,\n",
    "               normalize_plurals= True).generate(words_clean)\n",
    "\n",
    "plt.imshow(wc, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.savefig('covid_clound.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análise Temporal dos Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tweets postados por dia\n",
    "data = tweets_df\n",
    "print(data['Date'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tweets postados por dia\n",
    "data = tweets_df\n",
    "\n",
    "print(data['Date'][0])\n",
    "\n",
    "data['Date'] = pd.to_datetime(data['Date']).apply(lambda x: x.date())\n",
    "\n",
    "print(data['Date'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data['Date'].value_counts() #conta a quantidade de tweets por dia\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tlen = pd.Series(data['Date'].value_counts(), index=data['Date'])\n",
    "\n",
    "tlen.plot(figsize=(16,4), color='r');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Mining com o pacote NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "tweet_tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_tokens = [] #lista para armazenar os tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tweet in tweets_df['Tweets']:   \n",
    "    print (tweet_tokenizer.tokenize(tweet))\n",
    "    tweets_tokens.append(tweet_tokenizer.tokenize(tweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords \n",
    "import string\n",
    " \n",
    "#punctuation = list(string.punctuation)\n",
    "stopwords_english = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tweets(tweet):\n",
    "    # remove stock market tickers like $GE\n",
    "    tweet = re.sub(r'\\$\\w*', '', tweet)\n",
    " \n",
    "    # remove old style retweet text \"RT\"\n",
    "    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
    " \n",
    "    # remove hyperlinks\n",
    "    tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n",
    "    \n",
    "    # remove hashtags\n",
    "    # only removing the hash # sign from the word\n",
    "    tweet = re.sub(r'#', '', tweet)\n",
    "    \n",
    "    # tokenize tweets\n",
    "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)\n",
    "    tweet_tokens = tokenizer.tokenize(tweet)\n",
    " \n",
    "    tweets_clean = []    \n",
    "    for word in tweet_tokens:\n",
    "        if (word not in stopwords_english and # remove stopwords\n",
    "                word not in string.punctuation): # remove punctuation\n",
    "            tweets_clean.append(word)\n",
    " \n",
    "    return tweets_clean   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_tweets_tokens = [] #lista para armazenar os tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tweet in tweets_df['Tweets']:   \n",
    "    print (clean_tweets(tweet))\n",
    "    clean_tweets_tokens.append(clean_tweets(tweet))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
